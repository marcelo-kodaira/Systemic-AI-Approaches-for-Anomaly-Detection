{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis for Anomaly Detection\n",
    "\n",
    "This notebook provides a beginner-friendly interface for:\n",
    "- Loading and exploring datasets\n",
    "- Visualizing data patterns\n",
    "- Running quick anomaly detection experiments\n",
    "- Understanding model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_blobs, make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import warnings\n",
    "\n",
    "# Import our custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data_preprocessing import DataPreprocessor, preprocess_pipeline\n",
    "from model_training import ModelTrainer, train_ensemble\n",
    "from evaluation import ModelEvaluator\n",
    "from drift_detection import ConceptDriftMonitor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data for Testing\n",
    "\n",
    "Let's create a synthetic dataset with normal data and anomalies for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate synthetic data with anomalies\n",
    "def create_anomaly_dataset(n_samples=1000, contamination=0.1, n_features=2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create synthetic dataset with normal data and anomalies.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Total number of samples\n",
    "        contamination: Proportion of anomalies\n",
    "        n_features: Number of features\n",
    "        random_state: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        X: Feature matrix\n",
    "        y: Labels (0 = normal, 1 = anomaly)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    n_inliers = int(n_samples * (1 - contamination))\n",
    "    n_outliers = n_samples - n_inliers\n",
    "    \n",
    "    # Generate normal data (inliers)\n",
    "    X_inliers = np.random.randn(n_inliers, n_features)\n",
    "    \n",
    "    # Generate anomalies (outliers) - far from normal data\n",
    "    X_outliers = np.random.uniform(low=-6, high=6, size=(n_outliers, n_features))\n",
    "    \n",
    "    # Combine data\n",
    "    X = np.vstack([X_inliers, X_outliers])\n",
    "    y = np.hstack([np.zeros(n_inliers), np.ones(n_outliers)])\n",
    "    \n",
    "    # Shuffle\n",
    "    shuffle_idx = np.random.permutation(n_samples)\n",
    "    X = X[shuffle_idx]\n",
    "    y = y[shuffle_idx]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Create dataset\n",
    "X, y = create_anomaly_dataset(n_samples=1000, contamination=0.1, n_features=2)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "df['is_anomaly'] = y\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of normal samples: {(y == 0).sum()}\")\n",
    "print(f\"Number of anomalies: {(y == 1).sum()}\")\n",
    "print(f\"Contamination rate: {y.mean():.2%}\")\n",
    "print(\"\\nFirst few samples:\")\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize data distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 2D scatter plot\n",
    "ax = axes[0, 0]\n",
    "normal = df[df['is_anomaly'] == 0]\n",
    "anomalies = df[df['is_anomaly'] == 1]\n",
    "ax.scatter(normal['feature_0'], normal['feature_1'], c='blue', alpha=0.6, label='Normal')\n",
    "ax.scatter(anomalies['feature_0'], anomalies['feature_1'], c='red', alpha=0.8, label='Anomaly', marker='x', s=50)\n",
    "ax.set_xlabel('Feature 0')\n",
    "ax.set_ylabel('Feature 1')\n",
    "ax.set_title('Data Distribution in 2D Space')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature distributions\n",
    "for i, col in enumerate(['feature_0', 'feature_1']):\n",
    "    ax = axes[0, 1] if i == 0 else axes[1, 0]\n",
    "    ax.hist(normal[col], bins=30, alpha=0.7, label='Normal', color='blue', density=True)\n",
    "    ax.hist(anomalies[col], bins=20, alpha=0.7, label='Anomaly', color='red', density=True)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'Distribution of {col}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation heatmap\n",
    "ax = axes[1, 1]\n",
    "correlation = df.drop('is_anomaly', axis=1).corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, ax=ax)\n",
    "ax.set_title('Feature Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize and apply preprocessing\n",
    "preprocessor = DataPreprocessor(\n",
    "    scaling_method='standard',\n",
    "    feature_selection_method=None  # No dimensionality reduction for 2D data\n",
    ")\n",
    "\n",
    "# Clean data (remove duplicates, handle missing values)\n",
    "df_clean = preprocessor.clean_data(df, drop_duplicates=True, handle_missing='drop')\n",
    "\n",
    "# Prepare features and labels\n",
    "X_clean = df_clean.drop('is_anomaly', axis=1).values\n",
    "y_clean = df_clean['is_anomaly'].values\n",
    "\n",
    "# Split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_clean, y_clean, test_size=0.3, random_state=42, stratify=y_clean\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "X_test_scaled = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")\n",
    "print(f\"Training anomaly rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test anomaly rate: {y_test.mean():.2%}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Anomaly Detection Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train Isolation Forest with hyperparameter optimization\n",
    "print(\"Training Isolation Forest with Optuna optimization...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trainer = ModelTrainer(\n",
    "    model_type='isolation_forest',\n",
    "    task_type='anomaly_detection',\n",
    "    n_trials=20  # Reduced for demo\n",
    ")\n",
    "\n",
    "# Optimize hyperparameters\n",
    "result = trainer.optimize(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f\"\\nBest parameters found:\")\n",
    "for param, value in result['best_params'].items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest validation score: {result['best_score']:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make predictions\n",
    "model = result['model']\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Convert predictions to binary (0 = normal, 1 = anomaly)\n",
    "# Isolation Forest returns -1 for anomalies, 1 for normal\n",
    "y_pred_binary = (y_pred == -1).astype(int)\n",
    "\n",
    "# Get anomaly scores\n",
    "scores = model.decision_function(X_test_scaled)\n",
    "\n",
    "print(f\"Predictions shape: {y_pred_binary.shape}\")\n",
    "print(f\"Predicted anomalies: {y_pred_binary.sum()}\")\n",
    "print(f\"Actual anomalies: {y_test.sum()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(task_type='anomaly_detection')\n",
    "\n",
    "# Compute metrics\n",
    "metrics = evaluator.compute_anomaly_metrics(y_test, y_pred_binary, scores)\n",
    "\n",
    "print(\"Anomaly Detection Performance Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "for metric, value in metrics.items():\n",
    "    if metric != 'confusion_matrix' and not isinstance(value, np.ndarray):\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Confusion Matrix\n",
    "ax = axes[0]\n",
    "cm = metrics['confusion_matrix']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Normal', 'Anomaly'],\n",
    "            yticklabels=['Normal', 'Anomaly'])\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "\n",
    "# Plot 2: Predictions visualization\n",
    "ax = axes[1]\n",
    "# Use original unscaled data for visualization\n",
    "colors = ['blue' if pred == 0 else 'red' for pred in y_pred_binary]\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=colors, alpha=0.6)\n",
    "ax.set_xlabel('Feature 0')\n",
    "ax.set_ylabel('Feature 1')\n",
    "ax.set_title('Predicted Anomalies')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='blue', label='Predicted Normal'),\n",
    "                  Patch(facecolor='red', label='Predicted Anomaly')]\n",
    "ax.legend(handles=legend_elements)\n",
    "\n",
    "# Plot 3: Score distribution\n",
    "ax = axes[2]\n",
    "normal_scores = scores[y_test == 0]\n",
    "anomaly_scores = scores[y_test == 1]\n",
    "ax.hist(normal_scores, bins=30, alpha=0.7, label='Normal', color='blue', density=True)\n",
    "ax.hist(anomaly_scores, bins=20, alpha=0.7, label='Anomaly', color='red', density=True)\n",
    "ax.set_xlabel('Anomaly Score')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Anomaly Score Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bootstrap Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate bootstrap confidence intervals\n",
    "print(\"Calculating Bootstrap Confidence Intervals...\")\n",
    "bootstrap_results = evaluator.bootstrap_confidence_intervals(\n",
    "    y_test, y_pred_binary,\n",
    "    n_bootstraps=100,  # Reduced for demo\n",
    "    confidence_level=0.95\n",
    ")\n",
    "\n",
    "print(f\"\\nF1 Score: {bootstrap_results['original_score']:.4f}\")\n",
    "print(f\"95% Confidence Interval: [{bootstrap_results['ci_lower']:.4f}, {bootstrap_results['ci_upper']:.4f}]\")\n",
    "print(f\"Bootstrap Mean: {bootstrap_results['mean']:.4f}\")\n",
    "print(f\"Bootstrap Std: {bootstrap_results['std']:.4f}\")\n",
    "\n",
    "# Visualize bootstrap distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(bootstrap_results['scores'], bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.axvline(bootstrap_results['original_score'], color='red', linestyle='--', linewidth=2, label='Original Score')\n",
    "plt.axvline(bootstrap_results['ci_lower'], color='green', linestyle=':', linewidth=2, label='95% CI Lower')\n",
    "plt.axvline(bootstrap_results['ci_upper'], color='green', linestyle=':', linewidth=2, label='95% CI Upper')\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bootstrap Distribution of F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train and Compare Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train ensemble of models\n",
    "print(\"Training ensemble of anomaly detection models...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "ensemble_results = train_ensemble(\n",
    "    X_train_scaled, y_train,\n",
    "    models=['isolation_forest', 'one_class_svm'],  # Reduced for demo\n",
    "    task_type='anomaly_detection',\n",
    "    n_trials=10  # Reduced for demo\n",
    ")\n",
    "\n",
    "print(f\"\\nBest model: {ensemble_results['best_model_type']}\")\n",
    "\n",
    "# Compare models\n",
    "model_scores = {}\n",
    "for model_name, model_info in ensemble_results['models'].items():\n",
    "    model = model_info['model']\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_binary = (y_pred == -1).astype(int)\n",
    "    \n",
    "    from sklearn.metrics import f1_score\n",
    "    score = f1_score(y_test, y_pred_binary)\n",
    "    model_scores[model_name] = score\n",
    "    print(f\"{model_name}: F1 Score = {score:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Drift Detection Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simulate data stream with concept drift\n",
    "def generate_stream_with_drift(n_batches=20, batch_size=50, drift_point=10):\n",
    "    \"\"\"\n",
    "    Generate data stream with concept drift.\n",
    "    \"\"\"\n",
    "    stream = []\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        if i < drift_point:\n",
    "            # Before drift: centered at origin\n",
    "            X_batch = np.random.randn(batch_size, 2)\n",
    "        else:\n",
    "            # After drift: shifted distribution\n",
    "            X_batch = np.random.randn(batch_size, 2) + [2, 2]\n",
    "        \n",
    "        stream.append(X_batch)\n",
    "    \n",
    "    return stream\n",
    "\n",
    "# Generate stream\n",
    "stream = generate_stream_with_drift(n_batches=20, batch_size=50, drift_point=10)\n",
    "\n",
    "# Initialize drift monitor\n",
    "drift_monitor = ConceptDriftMonitor(methods=['statistical'])\n",
    "\n",
    "# Process stream\n",
    "drift_detected = []\n",
    "for i, batch in enumerate(stream):\n",
    "    results = drift_monitor.update_unsupervised(batch)\n",
    "    if any(results.values()):\n",
    "        drift_detected.append(i)\n",
    "        print(f\"Drift detected at batch {i}!\")\n",
    "\n",
    "# Visualize drift\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot data stream\n",
    "ax = axes[0]\n",
    "for i, batch in enumerate(stream[:10]):\n",
    "    ax.scatter(batch[:, 0], batch[:, 1], alpha=0.3, label=f'Batch {i}' if i < 2 else '')\n",
    "ax.set_title('Data Stream Before Drift')\n",
    "ax.set_xlabel('Feature 0')\n",
    "ax.set_ylabel('Feature 1')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "for i, batch in enumerate(stream[10:15]):\n",
    "    ax.scatter(batch[:, 0], batch[:, 1], alpha=0.3, label=f'Batch {i+10}' if i < 2 else '')\n",
    "ax.set_title('Data Stream After Drift')\n",
    "ax.set_xlabel('Feature 0')\n",
    "ax.set_ylabel('Feature 1')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Get drift report\n",
    "drift_report = drift_monitor.get_drift_report()\n",
    "print(\"\\nDrift Detection Report:\")\n",
    "print(\"=\" * 50)\n",
    "for detector_name, info in drift_report['detectors'].items():\n",
    "    print(f\"{detector_name}:\")\n",
    "    print(f\"  Number of drifts: {info['n_drifts']}\")\n",
    "    print(f\"  Drift points: {info['drift_points']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Your Own Data\n",
    "\n",
    "Use this section to load and analyze your own datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: Load CSV file\n",
    "# Uncomment and modify the path to load your own data\n",
    "\"\"\"\n",
    "# Load your data\n",
    "your_data = pd.read_csv('../data/raw/your_dataset.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {your_data.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(your_data.head())\n",
    "\n",
    "# Preprocess your data\n",
    "preprocessor = DataPreprocessor(scaling_method='standard')\n",
    "your_data_clean = preprocessor.clean_data(your_data)\n",
    "\n",
    "# Continue with analysis...\n",
    "\"\"\"\n",
    "\n",
    "print(\"To load your own data:\")\n",
    "print(\"1. Place your CSV file in the '../data/raw/' directory\")\n",
    "print(\"2. Uncomment the code above and modify the file path\")\n",
    "print(\"3. Run the cell to load and analyze your data\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the trained model\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = '../models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = f\"{models_dir}/isolation_forest_{timestamp}.pkl\"\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save preprocessor\n",
    "preprocessor_path = f\"{models_dir}/preprocessor_{timestamp}.pkl\"\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f\"Preprocessor saved to: {preprocessor_path}\")\n",
    "\n",
    "# Save evaluation results\n",
    "results_dir = '../data/processed'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'metric': list(model_scores.keys()),\n",
    "    'score': list(model_scores.values())\n",
    "})\n",
    "results_path = f\"{results_dir}/model_comparison_{timestamp}.csv\"\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"Results saved to: {results_path}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. **Data Generation**: Creating synthetic anomaly detection datasets\n",
    "2. **Data Exploration**: Visualizing and understanding data patterns\n",
    "3. **Preprocessing**: Cleaning and scaling data\n",
    "4. **Model Training**: Using Optuna for hyperparameter optimization\n",
    "5. **Evaluation**: Computing comprehensive metrics and confidence intervals\n",
    "6. **Model Comparison**: Training and comparing multiple algorithms\n",
    "7. **Drift Detection**: Monitoring for concept drift in data streams\n",
    "\n",
    "### Next Steps:\n",
    "- Load your own dataset and apply the same analysis pipeline\n",
    "- Experiment with different preprocessing techniques\n",
    "- Try other anomaly detection algorithms\n",
    "- Adjust hyperparameter optimization settings\n",
    "- Implement real-time anomaly detection for streaming data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}